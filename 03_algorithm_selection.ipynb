{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6d58f1-20fc-4368-93d7-11f47cdd2cef",
   "metadata": {},
   "source": [
    "# Algorithm & Metric Deep Dive - Making appropriate choices for your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb798bbe-e4a4-4da9-9539-3a4065ca6572",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview \n",
    "\n",
    "In the previous notebook, we built a machine learning containing all the important elements we would find in any sophisticated, real world pipeline, but in our example we used quite simple components for each step. Even still, there were quite a number of *hyperparameters* we had to choose along the way, without having understood what we were choosing and why, or more importantly, what would be a good choice for our particular problem and the dataset. This notebook will attempt to give a little insight into how key ML algorithms work with aim of giving some basic understanding of what the hyperparameters mean and how they influence the end result. This will hopefully be a starting point for making approproate chopices of algorithm, hyperparamters and metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174fd29-1d97-435d-b5df-f7afbc70a7d2",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "* Completed notebooks 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db6639-c81c-438b-a792-f4ed8ca27f0f",
   "metadata": {},
   "source": [
    "### Learning Outcomes \n",
    "\n",
    "* Understand mechanisms of key tree based and neural network algorithms\n",
    "* Understand key hyperparameters and how to choose them\n",
    "* Understand key metrics and how to select the right one for your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af0dd1-fc66-4b0f-8f29-ca2a8568c02a",
   "metadata": {},
   "source": [
    "### Links to Best practices and Values\n",
    "* ML Pitfalls - Avoid problems such as overfitting and underfitting through appropriate choice of algorithms and hyperparameters\n",
    "* Ethics - Be able to justify your choices for what you have implemented.\n",
    "* ML Lifecycle - Ensure you are able to reproduce results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee931066-ee2a-447b-bbdb-267fbeeb4e47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Science Framework\n",
    "* Weather regimes - Discovery and Attribution\n",
    "* Radiation emulation - Fusing Simulation and Data Science\n",
    "* XBT - Uncertainty and Trust\n",
    "* Rotors - Data to Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f980f2-1d40-41d2-974c-aef97592e7af",
   "metadata": {},
   "source": [
    "## Tutorial - Decision Trees\n",
    "\n",
    "![An example of manually created flowchart for making decisions, similar to the structure of the decision tree.](xbt_imeta_flowchart.png)\n",
    "\n",
    "Root node: The base of the decision tree.\n",
    "Splitting: The process of dividing a node into multiple sub-nodes.\n",
    "Decision node: When a sub-node is further split into additional sub-nodes.\n",
    "Leaf node: When a sub-node does not further split into additional sub-nodes; represents possible outcomes.\n",
    "Pruning: The process of removing sub-nodes of a decision tree.\n",
    "Branch: A subsection of the decision tree consisting of multiple nodes.\n",
    "Hyoerparameters\n",
    "Max depth\n",
    "Image of tree https://miro.medium.com/max/1400/1*3P1333UmqEww6YMpjisj4Q.png from article https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6 \n",
    "\n",
    "Maths of CART algorithm\n",
    "\n",
    "https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\n",
    "How does this relate to decision trees hyperparameters\n",
    "\n",
    "In the decision tree, the nodes are split into subnodes on the basis of a threshold value of an attribute. The CART algorithm does that by searching for the best homogeneity for the subnodes, with the help of the Gini Index criterion. \n",
    "Quote from https://www.analyticssteps.com/blogs/classification-and-regression-tree-cart-algorithm  \n",
    "Training terms\n",
    "* Greedy algorithm\n",
    "* Stopping criteria\n",
    "* Pruning\n",
    "\n",
    "Use XBT example for trees\n",
    "Show XBT flowchart\n",
    "Example code\n",
    "Régression va classification trees\n",
    "\n",
    "Décision tree visualisation\n",
    "Variants:\n",
    "• random forest\n",
    "• Gradient boosted\n",
    "• Xgboost \n",
    "Random forest discussion \n",
    "Deals with variance Leo breiman\n",
    "Key terms\n",
    "Bgging\n",
    "Bootstapping\n",
    "aggregation\n",
    "Ensemble model\n",
    "\n",
    "\n",
    "Advantages\n",
    "\n",
    "Works for numerical or categorical data and variables.\n",
    "Models problems with multiple outputs.\n",
    "Tests the reliability of the tree.\n",
    "Requires less data cleaning than other data modeling techniques. Easy to explain to those without an analytical background.\n",
    "\n",
    "Disadvantages\n",
    "Affected by noise in the data.\n",
    "Not ideal for large datasets.\n",
    "Can disproportionately value, or weigh, attributes.\n",
    "The decisions at nodes are limited to binary outcomes, reducing the complexity that the tree can handle. Trees can become very complex when dealing with uncertainty and numerous linked outcomes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ea8c6-2ead-41b3-b7c3-71b946891c9e",
   "metadata": {},
   "source": [
    "# Example: Decision Trees - XBT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e73c9103-8a88-4bf5-a6fd-48a05597b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import functools\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73b0314-cc10-44ea-8409-1aefe5b1ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e77bb0-1ae3-478b-bcf3-463381f6d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4870b309-b6f9-4a81-9d44-984d2af0771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.tree\n",
    "import sklearn.preprocessing\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78392c-694f-49a1-9347-35983ad1c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_data_loc = pathlib.Path('/project/informatics_lab/xbt')\n",
    "xbt_fname_template = 'xbt_{year}.csv'\n",
    "year_range= (1966,2015)\n",
    "xbt_df = pandas.concat([pandas.read_csv(xbt_data_loc / xbt_fname_template.format(year=year1)) for year1 in range(year_range[0], year_range[1])])\n",
    "xbt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ae3a2-3b22-4c8e-b787-5be72ccbde95",
   "metadata": {},
   "source": [
    "#### Clean dataset\n",
    "Remove bad data points from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954082e-ae95-45f3-aaf3-93881f5e4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df = xbt_df[~((xbt_df['max_depth'] < 0) | (xbt_df['max_depth'].isna()))]\n",
    "xbt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d54f6-ebcd-4622-86cf-34493d6d38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'instrument'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69d970-ae1b-4e37-ad7c-3e4aad3e990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df[target_feature].value_counts().index[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96275645-9a21-4419-81f3-664342fa6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df = xbt_df[~(xbt_df[target_feature].str.contains('UNKNOWN'))]\n",
    "xbt_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1551f1-9a1d-44d0-8f36-f905cec21ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_df = xbt_df[xbt_df[target_feature].isin(list(xbt_df[target_feature].value_counts().index[:12]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e363bf-6d3e-451c-bd78-289a5d6416ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_labelled = xbt_df[xbt_df['imeta_applied'] == 0]\n",
    "xbt_unlabelled = xbt_df[xbt_df['imeta_applied'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ffbc5-c1e6-471e-a6e4-59a9fab213d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_labelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142f623-20f9-421e-8ffa-252e3f982877",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_unlabelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a597d-462a-4a2c-9c23-d3304ced7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_train, xbt_test = sklearn.model_selection.train_test_split(xbt_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af060a4-ab32-48b0-8b20-372a76433306",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {\n",
    "    'year': sklearn.preprocessing.MinMaxScaler(),\n",
    "    'max_depth': sklearn.preprocessing.MinMaxScaler(),\n",
    "    'lat': sklearn.preprocessing.MinMaxScaler(),\n",
    "    'lon': sklearn.preprocessing.MinMaxScaler(),\n",
    "}\n",
    "input_features = [list(scaler_dict.keys())]\n",
    "\n",
    "preproc_input_features = []\n",
    "for feature_name, scaler1 in scaler_dict.items():\n",
    "    scaler1.fit(xbt_train[[feature_name]])\n",
    "    preproc_input_features += [scaler1.transform(xbt_train[[feature_name]])]\n",
    "    \n",
    "X_train = numpy.concatenate( preproc_input_features, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34125b8d-4869-4de5-820e-6f184255d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(xbt_train[target_feature])\n",
    "y_train = target_encoder.transform(xbt_train[target_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e10eb-2a6e-432d-8027-0f1d323f3942",
   "metadata": {},
   "source": [
    "We can get the hyperparameters for our decision tree by creating a decision tree object. You can get more explanation from `help(sklearn.tree.DecisionTreeClassifier`, or from the [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159abc6e-13bf-4abb-b8ae-ce047fb0b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.tree.DecisionTreeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383a5ba-3ba4-4bdd-beab-977eefefe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_clf = sklearn.tree.DecisionTreeClassifier(\n",
    "    max_depth=5, # reduce chance of overfitting\n",
    "    min_samples_leaf= 2, #ensure that there won't be too small a number of samples in a leaf node\n",
    "    min_samples_split= 5, # ensure more sample at a node when it splits\n",
    ")\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bf4de-51cc-4325-a71e-4843239bab21",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99691df-7b5f-4724-8415-9881f80e38ec",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab12db-3995-48ee-b65e-16dbba75a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(64,64))\n",
    "_ = sklearn.tree.plot_tree(dt_clf)\n",
    "matplotlib.pyplot.show()\n",
    "fig1.savefig('treevis.svg',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db514ea-cf80-46ed-8a01-6cd4d5ad779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyperparameters = {\n",
    "    'max_depth' : 5, # reduce chance of overfitting\n",
    "    'min_samples_leaf' :  2, #ensure that there won't be too small a number of samples in a leaf node\n",
    "    'min_samples_split' :  5, # ensure more sample at a node when it splits\n",
    "    'n_estimators' : 20, # restrict number of trees in forest\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f647f39-580e-4e8e-89ff-f59ece58a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_clf = sklearn.ensemble.RandomForestClassifier(\n",
    "    **random_forest_hyperparameters\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea625a1-726b-4c13-a1e7-afd9b2f7b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p1,param_val in rf_clf.get_params().items():\n",
    "    print(f'param value {p1}={param_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be4895-11b6-427f-8dc7-2ee20bdc7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0e37b-5926-4e3d-86ce-2b2808637d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results from random forest and decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939c278-7680-4686-b055-808ddfb5313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = numpy.concatenate(\n",
    "    [scaler1.transform(xbt_test[[feature_name]]) for feature_name, scaler1 in scaler_dict.items()],\n",
    "    axis=1)\n",
    "y_test = target_encoder.transform(xbt_test[target_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ae1e4-f7cd-491b-a7c7-54c7f254f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc8863-e1c6-406f-ac14-6298924dd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(16,8))\n",
    "ax1 = fig1.add_subplot(1,2,1, title='frequency of different labels for decision tree predictions.')\n",
    "pandas.Series(target_encoder.inverse_transform(y_pred_dt)).value_counts().plot.pie(ax=ax1)\n",
    "ax1 = fig1.add_subplot(1,2,2, title='frequency of different labels for random forest predictions.')\n",
    "pandas.Series(target_encoder.inverse_transform(y_pred_rf)).value_counts().plot.pie(ax=ax1)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2df379-3683-481c-a6d8-fd8f3e858d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(target_encoder.classes_)), len(list(sklearn.metrics.precision_score(y_test, y_pred_dt, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe00114-ca18-4366-91c3-b119330e245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt_train['instrument'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49b813-464f-4999-9ede-3d205fdf67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_dt, recall_dt, f1_dt, support_dt = sklearn.metrics.precision_recall_fscore_support(y_test, y_pred_dt, average=None)\n",
    "prec_rf, recall_rf, f1_rf, support_rf = sklearn.metrics.precision_recall_fscore_support(y_test, y_pred_rf, average=None)\n",
    "metrics_xbt = pandas.DataFrame({\n",
    "    'classes': list(target_encoder.classes_),\n",
    "    'precision_dt': list(prec_dt),\n",
    "    'precision_rf': list(prec_rf),\n",
    "    'recall_dt': list(recall_dt),\n",
    "    'recall_rf': list(recall_rf),\n",
    "    'f1_dt': list(f1_dt),\n",
    "    'f1_rf': list(f1_rf),\n",
    "    'support_dt': list(support_dt),\n",
    "    'support_rf': list(support_rf),  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18243b-cf55-4c24-884f-fb19141d4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(20,30))\n",
    "ax1 = fig1.add_subplot(2,2,1)\n",
    "metrics_xbt.plot.bar(x='classes',y=['recall_dt','recall_rf'],ax=ax1)\n",
    "ax1 = fig1.add_subplot(2,2,2)\n",
    "metrics_xbt.plot.bar(x='classes',y=['precision_dt','precision_rf'],ax=ax1)\n",
    "ax1 = fig1.add_subplot(2,2,3)\n",
    "metrics_xbt.plot.bar(x='classes',y=['f1_dt','f1_rf'],ax=ax1)\n",
    "ax1 = fig1.add_subplot(2,2,4)\n",
    "metrics_xbt.plot.bar(x='classes',y=['support_dt','support_rf'],ax=ax1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28f9d5-68b8-46b8-896f-f5a1cfcf2075",
   "metadata": {},
   "source": [
    "## Tutorial - Neural Networks\n",
    "\n",
    "Description of Neural networks\n",
    "* Explain a single perceptron\n",
    "* History\n",
    "* Weighted sum plus threshold (binary activation)\n",
    "* Non linear activation, sigmoid neuron\n",
    "\n",
    "Key terms\n",
    "* neuron\n",
    "* Perceptron\n",
    "* Activation\n",
    "* Weights\n",
    "* Bias - the constant term\n",
    "* Sigmoid function\n",
    "\n",
    "Multi layer\n",
    "* How are they joined together?\n",
    "\n",
    "How do we train? \n",
    "* Gradient descent\n",
    "* Stochastic gradient decscent\n",
    "* Back propagation \n",
    "\n",
    "Key terms\n",
    "* gradient descent\n",
    "* Learning rate\n",
    "* back propagation\n",
    "* mini batch\n",
    "* epoch\n",
    "* copst function\n",
    "\n",
    "Hyperparameters\n",
    "* batch size\n",
    "* learning rate\n",
    "* solver\n",
    "* maximum iterations\n",
    "\n",
    " \n",
    "Types of NN\n",
    "* feed forward\n",
    "* Convolutional Neural Network\n",
    "* Recurrent Neural Network\n",
    "* Graphical Neural Network\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21b491-83a5-49c7-b4b0-72fc290044a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a9ee1f-3c6d-4a4f-8117-a6720ed2337f",
   "metadata": {},
   "source": [
    "## Example - Scorates Radiation Model Emulation\n",
    "\n",
    "Explain problem and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b38fd4b-ef1d-4929-bc87-373030a9bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import keras\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.layers \n",
    "import tensorflow.keras.layers \n",
    "import tensorflow.keras.layers \n",
    "import tensorflow.keras.models \n",
    "import code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80c7d2-de48-4eac-ac14-e81907c9c0b4",
   "metadata": {},
   "source": [
    "## Define inputs\n",
    "Specify the hyperparametersfor the pipeline and the location of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d47a88f-7c9e-4148-8125-f0fb9e288531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir   = pathlib.Path('/project/informatics_lab/data_science_cop/socrates_emulation/')\n",
    "output_dir = pathlib.Path(os.environ['SCRATCH']) / 'ml_weather_tutorial'\n",
    "\n",
    "if not output_dir.is_dir():\n",
    "    output_dir.mkdir()\n",
    "    print(f'creating directory {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121ef05-b41c-4c83-a950-86b99a8e662c",
   "metadata": {},
   "source": [
    "Set up the hyperparameters for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fb16076-85ab-49e9-9a4e-98d437da5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl='sw'\n",
    "target='nflx'\n",
    "nsamps = '50.0K'\n",
    "scale_data = True\n",
    "if wl=='sw':\n",
    "  model = 'sw_260'\n",
    "  model_ref = 'sw_ga7'\n",
    "elif wl=='lw':\n",
    "  model = 'lw_300'\n",
    "  model_ref = 'lw_ga7'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dec71a-599e-41a2-b200-9cd6f6157970",
   "metadata": {},
   "source": [
    "Construct the paths to file names that contain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1612a73b-7e99-43e9-a09b-72d2d78184df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root dir: /project/informatics_lab/data_science_cop/socrates_emulation\n"
     ]
    }
   ],
   "source": [
    "fnext='train'\n",
    "fn_meta = model+'_meta_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_levs = model+'_dat_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_lays = model+'_dat_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_surf = model+'_dat_surf_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='nflx':\n",
    "  fn_trg = model+'_trg_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='ndiv':\n",
    "  fn_trg = model+'_trg_lays_'+nsamps+'_'+fnext+'.npz'\n",
    " \n",
    "fnext='test'\n",
    "fn_meta_test = model+'_meta_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_levs_test = model+'_dat_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_lays_test = model+'_dat_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "fn_dat_surf_test = model+'_dat_surf_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='nflx':\n",
    "  fn_trg_test = model+'_trg_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "  fn_trg_ref = model_ref+'_trg_levs_'+nsamps+'_'+fnext+'.npz'\n",
    "if target=='ndiv':\n",
    "  fn_trg_test = model+'_trg_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "  fn_trg_ref = model_ref+'_trg_lays_'+nsamps+'_'+fnext+'.npz'\n",
    "\n",
    "print('root dir:',data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecd226-2fc8-4ad2-b90c-817c1fefc9c0",
   "metadata": {},
   "source": [
    "### Loading and preparing the  training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43a1a64f-dd58-4400-9b51-896f17f22bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sw_260_dat_lays_50.0K_train.npz\n",
      "loading sw_260_dat_surf_50.0K_train.npz\n",
      "loading sw_260_trg_levs_50.0K_train.npz\n"
     ]
    }
   ],
   "source": [
    "print('loading',fn_dat_lays)\n",
    "with numpy.load(data_dir / fn_dat_lays) as npzfile:\n",
    "    dat_lays = npzfile['dat_lays']\n",
    "    \n",
    "print('loading',fn_dat_surf)\n",
    "with numpy.load(data_dir / fn_dat_surf) as npzfile:\n",
    "    dat_surf = npzfile['dat_surf']\n",
    "    \n",
    "print('loading',fn_trg)\n",
    "with numpy.load(data_dir / fn_trg) as npzfile:\n",
    "    if target=='nflx':\n",
    "         trg = npzfile['trg_levs']\n",
    "    elif target=='ndiv':\n",
    "         trg = npzfile['trg_lays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "463356e2-60f7-40cf-aec5-23eb74a05b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = trg.shape[0]\n",
    "nlays = dat_lays.shape[1]\n",
    "nlay_feats = dat_lays.shape[2]\n",
    "nsurf_feats = dat_surf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09586ce4-57c3-4b9b-9d67-a8b2aa1e8ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 70, 35)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_lays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5fb8e27-4066-4572-941a-75451ee538a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing constant layer features: 6\n"
     ]
    }
   ],
   "source": [
    "if scale_data: # normalize by range\n",
    "  scaler_lays = []\n",
    "  use_lays = []\n",
    "  for ic in range(nlay_feats):\n",
    "    min0 = numpy.min(dat_lays[:,:,ic])\n",
    "    range0 = numpy.max(dat_lays[:,:,ic]) - min0\n",
    "    if range0 > 0.:\n",
    "      dat_lays[:,:,ic] = (dat_lays[:,:,ic] - min0)/range0\n",
    "      scaler_lays.append([min0, range0])\n",
    "      use_lays.append(ic)\n",
    "  if len(use_lays)<nlay_feats:\n",
    "    print('removing constant layer features:', nlay_feats-len(use_lays))\n",
    "    dat_lays = dat_lays[:,:,use_lays]\n",
    "    nlay_feats = len(use_lays)\n",
    "      \n",
    "\n",
    "  scaler_surf = []\n",
    "  use_surf = []\n",
    "  for ic in range(nsurf_feats):\n",
    "    min0 = numpy.min(dat_surf[:,ic])\n",
    "    range0 = numpy.max(dat_surf[:,ic]) - min0\n",
    "    if range0 > 0.:\n",
    "      dat_surf[:,ic] = (dat_surf[:,ic] - min0)/range0\n",
    "      scaler_surf.append([min0, range0])\n",
    "      use_surf.append(ic)\n",
    "  if len(use_surf)<nsurf_feats:\n",
    "    print('removing constant surf features:', nsurf_feats-len(use_surf))\n",
    "    dat_surf = dat_surf[:,:,use_surf]\n",
    "    nsurf_feats = len(use_surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef5a3c17-2dba-433c-8b90-98c1a532e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrg_samps = trg.shape[0]\n",
    "\n",
    "if target=='nflx' or target=='ndiv':\n",
    "  nouts=1\n",
    "  ntrg_levs = trg.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86089c56-6f6e-44c0-98ee-96f635548d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_mlp(nlays, nlay_feats):\n",
    "    profile_input = tensorflow.keras.layers.Input(shape=(nlays, nlay_feats), name='profile_input')\n",
    "    surf_input = tensorflow.keras.layers.Input(shape=(nsurf_feats,), name='surf_input')\n",
    "    flat_profs = tensorflow.keras.layers.Flatten()(profile_input)\n",
    "    raw_in = tensorflow.keras.layers.concatenate([flat_profs, surf_input])\n",
    "    raw_size = (nlays*nlay_feats)+nsurf_feats\n",
    "    prof_size = nlays*nlay_feats\n",
    "\n",
    "    x = tensorflow.keras.layers.Dense(512, use_bias=False, activation='relu')(raw_in)\n",
    "    x = tensorflow.keras.layers.Dense(512, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(256, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(256, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n",
    "\n",
    "    main_output = tensorflow.keras.layers.Dense(ntrg_levs, use_bias=True, activation='linear', name='main_output')(x)\n",
    "    model = tensorflow.keras.models.Model(inputs=[profile_input, surf_input], outputs=[main_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "74e0d400-b092-40a5-bc2e-4b05354af9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cnn(nlays, nlay_feats):\n",
    "    profile_input = tensorflow.keras.layers.Input(shape=(nlays, nlay_feats), name='profile_input')\n",
    "    surf_input = tensorflow.keras.layers.Input(shape=(nsurf_feats,), name='surf_input')\n",
    "    flat_profs = tensorflow.keras.layers.Flatten()(profile_input)\n",
    "    raw_in = tensorflow.keras.layers.concatenate([flat_profs, surf_input])\n",
    "    raw_size = (nlays*nlay_feats)+nsurf_feats\n",
    "    prof_size = nlays*nlay_feats\n",
    "\n",
    "    out = tensorflow.keras.layers.ZeroPadding1D(padding=1)(profile_input)\n",
    "    out = tensorflow.keras.layers.Conv1D(32, 3, strides=1, activation='relu', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\n",
    "    ident = out\n",
    "    out = tensorflow.keras.layers.ZeroPadding1D(padding=1)(out)\n",
    "    out = tensorflow.keras.layers.Conv1D(32, 3, strides=1, activation='relu', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\n",
    "    out = tensorflow.keras.layers.ZeroPadding1D(padding=1)(out)\n",
    "    out = tensorflow.keras.layers.Conv1D(32, 3, strides=1, activation='relu', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\n",
    "    x = tensorflow.keras.layers.add([out, ident])\n",
    "    out = tensorflow.keras.layers.Flatten()(x)\n",
    "    out = tensorflow.keras.layers.Dense(prof_size, use_bias=False, activation='relu')(out)\n",
    "\n",
    "    out = tensorflow.keras.layers.concatenate([out, surf_input])\n",
    "    x = tensorflow.keras.layers.add([out, raw_in])\n",
    "    x = tensorflow.keras.layers.Dense(1024, use_bias=False, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(1024, use_bias=False, activation='relu')(x)\n",
    "\n",
    "    main_output = tensorflow.keras.layers.Dense(ntrg_levs, use_bias=True, activation='linear', name='main_output')(x)\n",
    "    model = tensorflow.keras.models.Model(inputs=[profile_input, surf_input], outputs=[main_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d1d0098-fdf0-41d9-b876-5c638d7ac330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'mlp': {'build_func': build_model_mlp,},\n",
    "              'cnn_1d': {'build_func': build_model_cnn,},\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eaef9962-ab06-4985-98c3-415b553c59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building and training model mlp\n",
      "building and training model cnn_1d\n",
      "CPU times: user 4min 39s, sys: 26.7 s, total: 5min 6s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for model_name in model_dict.keys():\n",
    "    print(f'building and training model {model_name}')\n",
    "    model_dict[model_name]['model_object'] = model_dict[model_name]['build_func'](nlays=nlays, nlay_feats=nlay_feats)\n",
    "    model_dict[model_name]['model_object'].compile(loss='mean_absolute_error',\n",
    "                                                   optimizer='adam')\n",
    "    model_dict[model_name]['model_object'].fit([dat_lays, dat_surf], trg, epochs=1, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1825e462-ad13-40c9-88da-c95a9e9ab329",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db5a77-b8ce-4d8a-b6fd-2925d06a1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, selected_model in model_dict.items():\n",
    "  predictions[model_name] = selected_model['model_object'].predict([dat_lays_test, dat_surf_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a213f-78a2-42d0-b7af-e095b2c80337",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "for model_name in model_dict.keys():\n",
    "    metrics_dict['me_p'] = np.zeros(ntrg_levs)\n",
    "    metrics_dict['me_ctl'] = np.zeros(ntrg_levs)\n",
    "    metrics_dict['mae_p'] = np.zeros(ntrg_levs)\n",
    "    metrics_dict['mae_ctl'] = np.zeros(ntrg_levs)\n",
    "    for ilev in range(ntrg_levs):\n",
    "      metrics_dict['me_p'][ilev] = np.mean(predictions[model_name][:,ilev] - trg_test[:,ilev])\n",
    "      metrics_dict['me_ctl'][ilev] = np.mean(trg_ref[:,ilev] - trg_test[:,ilev])\n",
    "      metrics_dict['mae_p'][ilev] = np.mean(np.abs(predictions[model_name][:,ilev] - trg_test[:,ilev]))\n",
    "      metrics_dict['mae_ctl'][ilev] = np.mean(np.abs(trg_ref[:,ilev] - trg_test[:,ilev]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b203ea6-1c7a-4297-9bed-fc6e4fc5c50a",
   "metadata": {},
   "source": [
    "### Visualise metrics\n",
    "\n",
    "Display the perfromance metrics for our trained algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c8a63-1080-4d69-8450-ca35c6dac757",
   "metadata": {},
   "outputs": [],
   "source": [
    "yax = numpy.arange(1,len(me_p[1:])+1)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de013f-997f-4faf-a8fd-7558213d7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = maplotlib.pyplot.figure('compare_NN_results', figsize=(16,6))\n",
    "for ix1,model_name in enumerate(model_dict.keys()):\n",
    "    ax1 = fig1.add_subplot(2,4,(4*ix1) + 1,title=f'results for {model_name}')\n",
    "    ax1.plot(me_p[1:],yax, '-r', label='ME emu')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')\n",
    "  \n",
    "    ax1 = fig1.add_subplot(2,4,(4*ix1) + 2, title=f'results for {model_name}')\n",
    "    ax1.plot(mae_p[1:],yax, '--r', label='MAE emu')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    ax1 = fig1.add_subplot(2,4, (4*ix1) + 3, title=f'results for {model_name}')\n",
    "    plt.plot(me_ctl[1:],yax, '-c', label='ME ga7')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    ax1 = fig1.add_subplot(2,4, (4*ix1) + 4, title=f'results for {model_name}')\n",
    "    ax1.plot(mae_ctl[1:],yax, '--c', label='MAE ga7')\n",
    "    ax1.set_xlabel('level')\n",
    "    ax1.set_ylabel('flux / flux div. difference')\n",
    "    ax1.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53155c-0d44-4ba1-b17a-b8bb40915548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb134b5-0533-4e2e-874d-b5fffa4199d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd95c22-4f47-4adb-a9d3-93cc839118b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ae66ab5-d1a9-4852-8735-86c2bb1fd491",
   "metadata": {},
   "source": [
    "### Testing the output\n",
    "\n",
    "Next we load in the test data and do inferenceto check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02735791-0a41-4674-9d09-182bcee24f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sw_260_dat_lays_50.0K_test.npz\n",
      "loading sw_260_dat_surf_50.0K_test.npz\n",
      "loading sw_260_trg_levs_50.0K_test.npz\n",
      "loading sw_ga7_trg_levs_50.0K_test.npz\n"
     ]
    }
   ],
   "source": [
    "print('loading',fn_dat_lays_test)\n",
    "with numpy.load(data_dir / fn_dat_lays_test) as npzfile:\n",
    "    dat_lays_test = npzfile['dat_lays']\n",
    "\n",
    "print('loading',fn_dat_surf_test)\n",
    "with numpy.load(data_dir / fn_dat_surf_test) as npzfile:\n",
    "    dat_surf_test = npzfile['dat_surf']\n",
    "\n",
    "print('loading',fn_trg_test)\n",
    "with numpy.load(data_dir / fn_trg_test)as npzfile:\n",
    "    if target=='nflx':\n",
    "        trg_test = npzfile['trg_levs']\n",
    "    elif target=='ndiv':\n",
    "         trg_test = npzfile['trg_lays']\n",
    "\n",
    "print('loading',fn_trg_ref)\n",
    "with numpy.load(data_dir / fn_trg_ref) as npzfile:\n",
    "    if target=='nflx':\n",
    "         trg_ref = npzfile['trg_levs_ref']\n",
    "    elif target=='ndiv':\n",
    "         trg_ref = npzfile['trg_lays_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5a33eeab-1ea6-4fb2-b328-e32a70f667cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test data\n",
    "if scale_data: # normalize by range\n",
    "  dat_lays_test = dat_lays_test[:,:,use_lays]\n",
    "  for ic in range(nlay_feats):\n",
    "    dat_lays_test[:,:,ic] = (dat_lays_test[:,:,ic] - scaler_lays[ic][0])/scaler_lays[ic][1]\n",
    "\n",
    "  dat_surf_test = dat_surf_test[:,use_surf]\n",
    "  for ic in range(nsurf_feats):\n",
    "    dat_surf_test[:,ic] = (dat_surf_test[:,ic] - scaler_surf[ic][0])/scaler_surf[ic][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07442e-a144-4926-8982-3ad350d93608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c0064-36d6-4366-9cb4-a905173eb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddfd09-7f52-4e77-96b6-e396f3a50cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e56988-45e6-44a8-ae72-8841cfb73607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f8893-db55-4da0-bee6-1a1c72ac2885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06fbf53-7998-4e67-8743-4f735e03df59",
   "metadata": {},
   "source": [
    "### Example - Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb8c5c2-6998-41a5-873d-4a3e2ca2280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    falklands_data_dir = os.environ['OPMET_ROTORS_DATA_ROOT']\n",
    "except KeyError:\n",
    "    falklands_data_dir = '/project/informatics_lab/data_science_cop/ML_challenges/2021_opmet_challenge'\n",
    "falklands_data_dir = pathlib.Path(falklands_data_dir) /  'Rotors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c50bc3b8-b01f-4caf-87fb-95c23c0188ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_data_fname = 'new_training.csv'\n",
    "falklands_data_path = falklands_data_dir / falklands_data_fname\n",
    "falklands_df = pandas.read_csv(falklands_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6424020-429b-43d6-813b-e2f266b087cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]\n",
    "target_feature_name = 'rotors_present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1acb91b-84dd-4c50-a163-d402bd085287",
   "metadata": {},
   "outputs": [],
   "source": [
    "falklands_df = falklands_df.rename({'Rotors 1 is true': target_feature_name},axis=1)\n",
    "falklands_df.loc[falklands_df[falklands_df[target_feature_name].isna()].index, target_feature_name] = 0\n",
    "falklands_df['DTG'] = pandas.to_datetime(falklands_df['DTG'])\n",
    "falklands_df = falklands_df.drop_duplicates(subset=['DTG'])\n",
    "falklands_df = falklands_df[~falklands_df['DTG'].isnull()]\n",
    "falklands_df = falklands_df[(falklands_df['wind_speed_obs'] >= 0.0) &\n",
    "                            (falklands_df['air_temp_obs'] >= 0.0) &\n",
    "                            (falklands_df['wind_direction_obs'] >= 0.0) &\n",
    "                            (falklands_df['dewpoint_obs'] >= 0.0) \n",
    "                           ]\n",
    "falklands_df = falklands_df.drop_duplicates(subset='DTG')\n",
    "falklands_df[target_feature_name]  = falklands_df[target_feature_name] .astype(bool)\n",
    "falklands_df['time'] = pandas.to_datetime(falklands_df['DTG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2188b0-59b8-40cc-b07a-efab21d83d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.cos(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]\n",
    "\n",
    "def get_u_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.sin(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04b57a26-4c4a-433d-b3c4-b1faa1425173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 2.07 s, total: 18.3 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u_feature_template = 'u_wind_{level_ix}'\n",
    "v_feature_template = 'v_wind_{level_ix}'\n",
    "u_wind_feature_names = []\n",
    "v_wind_features_names = []\n",
    "for wsn1, wdn1 in zip(wind_speed_feature_names, wind_direction_feature_names):\n",
    "    level_ix = int( wsn1.split('_')[1])\n",
    "    u_feature = u_feature_template.format(level_ix=level_ix)\n",
    "    u_wind_feature_names += [u_feature]\n",
    "    falklands_df[u_feature] = falklands_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns')\n",
    "    v_feature = v_feature_template.format(level_ix=level_ix)\n",
    "    v_wind_features_names += [v_feature]\n",
    "    falklands_df[v_feature] = falklands_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f69909-cb92-49e5-a991-72643d61f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_train_df = falklands_df[falklands_df['time'] < datetime.datetime(2020,1,1,0,0)]\n",
    "rotors_test_df = falklands_df[falklands_df['time'] > datetime.datetime(2020,1,1,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403889d1-adb2-49b3-858c-c089407fb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_input(data_subset, pp_dict):\n",
    "    return numpy.concatenate([scaler1.transform(data_subset[[if1]]) for if1,scaler1 in pp_dict.items()],axis=1)\n",
    "\n",
    "def preproc_target(data_subset, enc1):\n",
    "     return enc1.transform(data_subset[[target_feature_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f078f090-a21e-4e3f-b036-d1c50405dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h01/shaddad/.conda/envs/ml-weather-tutorial-tf/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feature_names = temp_feature_names + humidity_feature_names + u_wind_feature_names + v_wind_features_names\n",
    "preproc_dict = {}\n",
    "for if1 in input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(rotors_train_df[[if1]])\n",
    "    preproc_dict[if1] = scaler1\n",
    "    \n",
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(rotors_train_df[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bb9d77f-3e82-4e58-b5f5-0009003fb881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h01/shaddad/.conda/envs/ml-weather-tutorial-tf/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train_rotors = preproc_input(rotors_train_df, preproc_dict)\n",
    "y_train_rotors = preproc_target(rotors_train_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caa42d00-5000-4496-ae18-e13e2fa4d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h01/shaddad/.conda/envs/ml-weather-tutorial-tf/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_test_rotors = preproc_input(rotors_test_df, preproc_dict)\n",
    "y_test_rotors = preproc_target(rotors_test_df, target_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b32b8af-8c64-42a4-8021-be7a4dd9bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate=2e-5\n",
    "drop_out_rate=0.2\n",
    "n_epochs=50\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15cb9821-8811-4591-92fe-e0d37ce0db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 300\n",
    "n_layers = 4\n",
    "inputs_shape=X_train_rotors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4558c59e-be48-401d-a6ad-07d0a22190f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ff_nn(n_layers, input_shape):\n",
    "    model = tensorflow.keras.models.Sequential()\n",
    "    model.add(tensorflow.keras.layers.Dropout(drop_out_rate, input_shape=(input_shape,)))\n",
    "    for i in numpy.arange(0,n_layers):\n",
    "        model.add(tensorflow.keras.layers.Dense(n_nodes, activation='relu', kernel_constraint=tensorflow.keras.constraints.max_norm(3)))\n",
    "        model.add(tensorflow.keras.layers.Dropout(drop_out_rate))\n",
    "    model.add(tensorflow.keras.layers.Dense(2, activation='softmax'))             # This is the output layer \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d41e5dc2-a4c1-444f-94ac-28460f5e8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 7.7 s, total: 2min 2s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rotors_ff_model = build_ff_nn(n_layers=n_layers, input_shape=inputs_shape)\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=initial_learning_rate)  \n",
    "rotors_ff_model.compile(optimizer=opt, loss='mse', metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
    "rotors_history=rotors_ff_model.fit(X_train_rotors, \n",
    "                                   y_train_rotors, \n",
    "                                   validation_data=(X_test_rotors, \n",
    "                                                    y_test_rotors), \n",
    "                                   epochs=n_epochs, \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=True,\n",
    "                                   verbose=False,\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bc6c527-fbf5-411f-a81b-ee170251c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_model(input_shape):\n",
    "    model = tensorflow.keras.Sequential()\n",
    "\n",
    "    model.add(tensorflow.keras.layers.Input(shape=(1,input_shape,)))\n",
    "    # Add a LSTM layer with 128 internal units.\n",
    "    model.add(tensorflow.keras.layers.LSTM(64,))\n",
    "\n",
    "    # Add a Dense layer with 10 units.\n",
    "    model.add(tensorflow.keras.layers.Dense(10))\n",
    "    \n",
    "    # add output layer\n",
    "    model.add(tensorflow.keras.layers.Dense(2, activation='softmax'))             # This is the output layer \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4a81caf-830e-407e-92aa-60cb1e19be61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 7.87 s, total: 1min 59s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "rotors_rnn_model = build_LSTM_model(input_shape=inputs_shape)\n",
    "opt = tensorflow.optimizers.Adam(learning_rate=initial_learning_rate)  \n",
    "rotors_rnn_model.compile(optimizer=opt, loss='mse', metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
    "rotors_history_rnn = rotors_ff_model.fit(X_train_rotors, \n",
    "                                         y_train_rotors, \n",
    "                                         validation_data=(X_test_rotors, \n",
    "                                                          y_test_rotors), \n",
    "                                         epochs=n_epochs, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True,         \n",
    "                                         verbose=False,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d8f31-1bce-4f9a-a653-51c9c10fa7a4",
   "metadata": {},
   "source": [
    "## Tutorial - Metrics\n",
    "\n",
    "\n",
    "Description of metrics\n",
    "* classification metrics\n",
    "* regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d843f-d6c6-4089-9207-826814820407",
   "metadata": {},
   "source": [
    "## Excercise - Metrics\n",
    "xx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89950120-33dd-4f51-87d2-b2d99672dcc0",
   "metadata": {},
   "source": [
    "## Examples of use\n",
    "xx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ba8b3-f653-497e-a8f3-cf86c30d3b54",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Rotor Challenge\n",
    "Radiation emulation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d6187-9d49-4722-b56b-77e05b1989b5",
   "metadata": {},
   "source": [
    "## Dataset Info\n",
    "* XBT Data\n",
    "* Radiation Emulation\n",
    "* Rotors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832efc6-7cb1-47d3-a7f2-89655afa20e8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "Decision trees\n",
    "* https://www.mastersindatascience.org/learning/introduction-to-machine-learning-algorithms/decision-tree/#:~:text=A\n",
    "* https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6\n",
    "* \n",
    "\n",
    "Neural Networks\n",
    "* [Introduction to Neural Networks - Kaggle](https://www.kaggle.com/code/carlosaguayo/introduction-to-neural-networks/notebook)\n",
    "* [Back propagation - wikipedia](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "*[Back propagation - brilliant wiki](https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights)\n",
    "* [Introduction to Deep Learning - Kaggle](https://www.kaggle.com/learn/intro-to-deep-learning)\n",
    "* [Introduction to Neural Networks - IBM](https://www.ibm.com/cloud/learn/neural-networks)\n",
    "* [Neural Networks - MIT](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414)\n",
    "\n",
    "* RNN\n",
    "* CNN\n",
    "* GNN\n",
    "\n",
    "Metrics\n",
    "* [Regression and Classification metrics - scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd136b7-29b8-4a85-8f93-63152b1c69c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-ml-weather-tutorial-tf Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-ml-weather-tutorial-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
